{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpsons Character Classifier \n",
    "\n",
    "<img src=\"pic_0003.jpg\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "## Goals:\n",
    "### 1. Reuse VGG 16  model to train on dog breed data\n",
    "### 2. Freeze most of the layers and fine tune few top layers\n",
    "### 3. Cut top layer and add new Fully Cunnected layer\n",
    "### 4. Retrain just top layers \n",
    "### 5. EValuate Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation,Dense\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications import vgg16\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import rand\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "# Scikit Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Matplot Imports\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import seaborn as sns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialize matplotlib parameters\n",
    "\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "%matplotlib inline\n",
    "\n",
    "# pandas display data frames as tables\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset\n",
    "\n",
    "The Simposons available at [Kaggle/Dogbreed](https://www.kaggle.com/alexattia/the-simpsons-characters-dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = '/Volumes/My Passport for Mac/data/simpsons/'\n",
    "test_folder = '/Volumes/My Passport for Mac/data/dog-breeds/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagesFilePathsFromFolder(path):\n",
    "    files = []\n",
    "    print(\"path: \" + path )\n",
    "    for f in listdir(path):\n",
    "        if( isfile(join(path, f)) and (\".jpg\" in f)) :\n",
    "            #print(\"path: \" + path)\n",
    "            #print(\"file:\" + f)\n",
    "            #print(\"path + file\", os.path.join(path, f))\n",
    "            files.append(os.path.join(path, f))  \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset of labels and image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "target_labels_list = []\n",
    "image_path_list = []\n",
    "counter = 0\n",
    "fileCounter = 0\n",
    "\n",
    "walkIterations = 0\n",
    "for r,d,f in os.walk(train_folder):\n",
    "    print(\"Walk Iteration : \" + str(walkIterations) )\n",
    "    print(\"root : \" + r)\n",
    "    print(\" dir: \" + str(d))\n",
    "    print(\"file: \" + str(f))\n",
    "    if(walkIterations == 0 ):\n",
    "        walkIterations = walkIterations + 1\n",
    "        continue\n",
    "    fileList = getImagesFilePathsFromFolder(r)\n",
    "    for file in fileList:\n",
    "        label = file.split(\"/\")[-2]\n",
    "        image_path = file\n",
    "        print(\"file : \" + file)\n",
    "        print(\"label : \" + label)\n",
    "        print(\"image_path : \" + image_path)\n",
    "        target_labels_list.append(label)\n",
    "        image_path_list.append(image_path)\n",
    "        counter = counter + 1\n",
    "        fileCounter = fileCounter + 1\n",
    "    walkIterations = walkIterations + 1\n",
    "    \n",
    "                \n",
    "print(counter)            \n",
    "data_dict[\"label\"] = target_labels_list            \n",
    "data_dict[\"image_path\"] = image_path_list\n",
    "input_df = pd.DataFrame(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Number of Classes in the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = input_df['label']\n",
    "len(set(target_labels))\n",
    "print(target_labels[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Labels\n",
    "Deep Learning models work with one hot encoded outputs or target variables. We utilize pandas to prepare one hot encoding for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ohe_names = pd.get_dummies(target_labels, sparse=True)\n",
    "\n",
    "print(labels_ohe_names.shape)\n",
    "labels_ohe = np.asarray(labels_ohe_names)\n",
    "labels_ohe_names.to_csv(\"labels.csv\")\n",
    "print(type(labels_ohe_names))\n",
    "print(labels_ohe.shape)\n",
    "print(labels_ohe[0:5])\n",
    "labels_ohe_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(color_codes=True)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(17.7, 25.27)\n",
    "sns.catplot(y=\"label\",   kind=\"count\", palette=\"ch:.25\",ax=ax, data=input_df, order=input_df.label.value_counts().iloc[:20].index);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train-Test Datasets\n",
    "We use a 70-30 split to prepare the two dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([img_to_array(\n",
    "                            load_img(img, \n",
    "                                     target_size=(299, 299))\n",
    "                       ) for img \n",
    "                           in input_df['image_path'].values.tolist()\n",
    "                      ]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, \n",
    "                                                    target_labels, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=np.array(target_labels), \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, \n",
    "                                                    y_train, \n",
    "                                                    test_size=0.15, \n",
    "                                                    stratify=np.array(y_train), \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare target variables for train, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ohe = pd.get_dummies(y_train.reset_index(drop=True)).as_matrix()\n",
    "y_val_ohe = pd.get_dummies(y_val.reset_index(drop=True)).as_matrix()\n",
    "y_test_ohe = pd.get_dummies(y_test.reset_index(drop=True)).as_matrix()\n",
    "\n",
    "y_train_ohe.shape, y_test_ohe.shape, y_val_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Since number of samples per class are not very high, we utilize data augmentation to prepare different variations of different samples available. We do this using the ```ImageDataGenerator utility``` from ```keras```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train generator.\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                   rotation_range=30, \n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, \n",
    "                                   horizontal_flip = 'true')\n",
    "train_generator = train_datagen.flow(x_train, y_train_ohe, shuffle=False, batch_size=BATCH_SIZE, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation generator\n",
    "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "val_generator = train_datagen.flow(x_val, y_val_ohe, shuffle=False, batch_size=BATCH_SIZE, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Deep Learning Classifier\n",
    "\n",
    "* Load InceptionV3 pretrained on ImageNet without its top/classification layer\n",
    "* Add additional custom layers on top of InceptionV3 to prepare custom classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the InceptionV3 model so we can do transfer learning\n",
    "#base_inception = InceptionV3(weights='imagenet', include_top = False, input_shape=(299, 299, 3))\n",
    "base_vgg16_model = vgg16.VGG16(weights='imagenet', include_top = False, input_shape=(299, 299, 3))\n",
    "\n",
    "#print(base_inception.summary())\n",
    "\n",
    "base_vgg16_model.trainable = True\n",
    "set_trainable = False\n",
    "for layer in base_vgg16_model.layers:\n",
    "    print(layer)\n",
    "    if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        print(layer.name)\n",
    "        print(\"set trainable as true\")\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False\n",
    "        print(\"set trainable as false\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a global spatial average pooling layer\n",
    "out = base_vgg16_model.output\n",
    "out = GlobalAveragePooling2D()(out)\n",
    "out = Dense(512, activation='relu')(out)\n",
    "out = Dense(512, activation='relu')(out)\n",
    "total_classes = y_train_ohe.shape[1]\n",
    "predictions = Dense(total_classes, activation='softmax')(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stack the two models (InceptionV3 and custom layers) on top of each other \n",
    "* Compile the model and view its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_vgg16_model.input, outputs=predictions)\n",
    "\n",
    "# only if we want to freeze layers\n",
    "#for layer in base_inception.layers:\n",
    "#    layer.trainable = False\n",
    "    \n",
    "model.summary()\n",
    "# Compile \n",
    "model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "We train the model with a Batch Size of 32 for just 15 Epochs.\n",
    "\n",
    "The model utilizes the power of transfer learning to achieve a validation accuracy of about __81%__ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = BATCH_SIZE\n",
    "train_steps_per_epoch = x_train.shape[0] // batch_size\n",
    "val_steps_per_epoch = x_val.shape[0] // batch_size\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=train_steps_per_epoch,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=val_steps_per_epoch,\n",
    "                              epochs=15,\n",
    "                              callbacks=[checkpoint],\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "t = f.suptitle('Deep Neural Net Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epochs = list(range(1,16))\n",
    "ax1.plot(epochs, history.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax1.set_xticks(epochs)\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epochs, history.history['loss'], label='Train Loss')\n",
    "ax2.plot(epochs, history.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(epochs)\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model Performance\n",
    "\n",
    "Step 1 is to prepare the training dataset. Since we scaled training data, test data should also be scaled in a similar manner. \n",
    "\n",
    "_Note: Deep Learning models are very sensitive to scaling._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling test features\n",
    "x_test /= 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(x_test)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(test_predictions, columns=labels_ohe_names.columns)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_test(img_path, img_height, img_width):\n",
    "    return np.array([img_to_array(load_img(img_path, target_size=(img_height, img_width)))]).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, model):\n",
    "    #x_test = kimage.resize(image, 299, 299) \n",
    "    x_test /= 255.0\n",
    "    test_predictions = model.predict(x_test)\n",
    "    predictions = pd.DataFrame(test_predictions, columns=labels_ohe_names.columns)\n",
    "    predictions = list(predictions.idxmax(axis=1))\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagesFilePathsFromFolder(path):\n",
    "    onlyfiles = [ join(path,f) for f in listdir(path) if ( isfile(join(path, f)) and (\".jpg\" in f) )]\n",
    "    return onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = list(y_test)\n",
    "predictions = list(predictions.idxmax(axis=1))\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_JSON_PATH = \"/Volumes/My Passport for Mac/model/simpsons.json\"\n",
    "MODEL_H5_PATH = \"/Volumes/My Passport for Mac/model/simpsons.h5\"\n",
    "\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(MODEL_JSON_PATH, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(MODEL_H5_PATH)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open(MODEL_JSON_PATH, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(MODEL_H5_PATH)\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(Adam(lr=.0001), loss='categorical_crossentropy', metrics=['accuracy']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test images and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Volumes/My Passport for Mac/data/simpsons-test\"\n",
    "fileCount = len(getImagesFilePathsFromFolder(path)) \n",
    "#print(getImagesFilePathsFromFolder(path))\n",
    "fig, ax = plt.subplots(1,fileCount, figsize=(50,50))\n",
    "img_Counter=0;\n",
    "\n",
    "for img_path in getImagesFilePathsFromFolder(path):\n",
    "    print(img_path)\n",
    "    breed = predict(get_x_test(img_path, 299, 299), loaded_model)[0]\n",
    "    print(breed)\n",
    "    ax[img_Counter].set_title(breed)\n",
    "    ax[img_Counter].imshow(load_img(img_path, target_size=(299, 299)))\n",
    "    img_Counter = img_Counter + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_evaluation_utils as meu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_classification_report(true_labels=test_labels, \n",
    "                                  predicted_labels=predictions, \n",
    "                                  classes=list(labels_ohe_names.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_confusion_matrix(true_labels=test_labels, \n",
    "                                    predicted_labels=predictions, \n",
    "                                    classes=list(labels_ohe_names.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves a test accuracy of approximately __86%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Model Performance\n",
    "Visualize model performance with actual images, labels and prediction confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_width = 5\n",
    "grid_height = 5\n",
    "f, ax = plt.subplots(grid_width, grid_height)\n",
    "f.set_size_inches(15, 15)\n",
    "batch_size = 25\n",
    "dataset = x_test\n",
    "\n",
    "label_dict = dict(enumerate(labels_ohe_names.columns.values))\n",
    "model_input_shape = (1,)+model.get_input_shape_at(0)[1:]\n",
    "random_batch_indx = np.random.permutation(np.arange(0,len(dataset)))[:batch_size]\n",
    "\n",
    "img_idx = 0\n",
    "for i in range(0, grid_width):\n",
    "    for j in range(0, grid_height):\n",
    "        actual_label = np.array(y_test)[random_batch_indx[img_idx]]\n",
    "        prediction = model.predict(dataset[random_batch_indx[img_idx]].reshape(model_input_shape))[0]\n",
    "        label_idx = np.argmax(prediction)\n",
    "        predicted_label = label_dict.get(label_idx)\n",
    "        conf = round(prediction[label_idx], 2)\n",
    "        ax[i][j].axis('off')\n",
    "        ax[i][j].set_title('Actual: '+actual_label+'\\nPred: '+predicted_label + '\\nConf: ' +str(conf))\n",
    "        ax[i][j].imshow(dataset[random_batch_indx[img_idx]])\n",
    "        img_idx += 1\n",
    "\n",
    "plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.5, hspace=0.55)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
